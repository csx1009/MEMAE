output:
    output_dir: '/data/csx/MAE/MAE/output_13_1000_442'
    sub: 'run'

model:
    model: 'mae_vit_base'
    epochs: 2000
    in_chans: 13
    batch_size: 6
    accum_iter: 1 #Accumulate gradient iterations (for increasing the effective batch size under memory constraints)
    patch_size: [4, 4, 5]
    mask_ratio: 0.75
    norm_pix_loss: False
    weight_decay: 0.05
    lr: 0.001
    blr: 0.0001 #base learning rate: absolute_lr = base_lr * total_batch_size / 256
    min_lr: 0.0001
    warmup_epochs: 7
    seed: 0
    resume: '' #resume from checkpoint
    load_partial: ['blocks']
    freeze_partial: False
    load_optim: True
    start_epoch: 0
    distributed: False
    num_workers: 40
    dist_on_itp: False #url used to set up distributed training
    pin_mem: True #Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU
    device: 'cuda:2'
    force_not_distributed: True
    save_every: 200