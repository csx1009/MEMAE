output:
    output_dir: './output/pretrain/rough_search'
    sub: 'run'

model:
    model: 'mae_vit_base'
    epochs: 100
    in_chans: 1
    batch_size: 4
    accum_iter: 1 #Accumulate gradient iterations (for increasing the effective batch size under memory constraints)
    patch_size: [8, 8, 4]
    mask_ratio: 0.85
    norm_pix_loss: False
    weight_decay: 0.05
    lr: 0.001
    blr: 0.0001 #base learning rate: absolute_lr = base_lr * total_batch_size / 256
    min_lr: 0.0001
    warmup_epochs: 7
    seed: 0
    resume: '' #resume from checkpoint
    load_partial: False #['blocks']
    freeze_partial: True
    load_optim: True
    start_epoch: 0
    distributed: False
    num_workers: 20
    dist_on_itp: False #url used to set up distributed training
    pin_mem: True #Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU
    device: 'cuda:1'
    save_every: 5

data:
    general:
        shape: [128, 128, 16]
        channel_first: True
        patch_file: './../../data/patch_data/NLST/NLST_size_180x180x96_space_1x1x3.h5'
        scale: [[-600., 200.], [0., 1.]]
        label_column: 'label'
        label_mapping: {}
        one_hot: False
    train:
        label_file: '../../data/label/NLST/split/train.csv'
        training: True
        augment:
            flip: True
            rot: True
            zoom: False
            offset: [-0.025, 0.025]
            noise: False
            blur: False
    valid:
        label_file: '../../data/label/NLST/split/valid.csv'
        training: False
        augment: []
